{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U OpenAI"
      ],
      "metadata": {
        "id": "nquyBwAJpkgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exclusive to colab environment. pip line for every session to install OpenAI\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls /content/drive/MyDrive/Leidos/demographic_generator/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6rQUrSeppuI",
        "outputId": "6706df81-3a09-4ab5-894e-49ebc3d9e93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "BadGuy.py      DemoGenJuly.py\t      demographics_sampling.py\tgeneration.py\t__pycache__\n",
            "DemoGenAug.py  demographic_generator  female_names.txt\t\tmale_names.txt\tsurnames.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac-_sGwzoG2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a347a82-662a-48fe-e260-1d899f162389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Name': ('Kendall', 'Dushkin'), 'Age': 21, 'Gender': 'man', 'Nationality': 'El Salvadoran', 'Items': 'an expired passport', 'Att': 'do not like authorities'}\n",
            "{'Name': ('Jefferson', 'Karmely'), 'Age': 35, 'Gender': 'man', 'Nationality': 'Paraguayan', 'Suspicion': 'no personal possessions of any kind'}\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import sys\n",
        "import pickle\n",
        "import threading\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import string\n",
        "import random\n",
        "string.punctuation += \"¿¡\"\n",
        "#sys.path.append('../demographic_generator/') #FOR LINUX TYPE OS\n",
        "sys.path.append('/content/drive/MyDrive/Leidos/demographic_generator/') #FOR GOOGLE COLAB\n",
        "from DemoGenAug import demographicsLoader #from '' will be the file name of demographic .py file\n",
        "os.environ[\"TZ\"]=\"US/Central\" #time modules used for generate unique filenames\n",
        "time.tzset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jdc # MODULE FOR HAVNNG CLASS FUNCTIONS ACROSS DIFFERENT CELLS\n",
        "!pip install ipdb # MODULE FOR DEBUGGING/ADDING BREAKPOINT IN GOOGLE COLAB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAyvjw6LQ37l",
        "outputId": "feb6e06e-d7d9-4679-b941-df9df495f831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jdc\n",
            "  Downloading jdc-0.0.9-py2.py3-none-any.whl.metadata (817 bytes)\n",
            "Downloading jdc-0.0.9-py2.py3-none-any.whl (2.1 kB)\n",
            "Installing collected packages: jdc\n",
            "Successfully installed jdc-0.0.9\n",
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jdc\n",
        "import ipdb # activate with: %pdb on\n",
        "class self_talk:\n",
        "  def __init__(self, top_p, temp, presence, PATH, key, freq=0.0):\n",
        "    self.top_p = top_p\n",
        "    self.temp = temp\n",
        "    self.freq = freq\n",
        "    self.presence = presence\n",
        "    self.PATH = PATH\n",
        "\n",
        "    self.client = OpenAI(api_key=key)\n",
        "    self.df = demographicsLoader()"
      ],
      "metadata": {
        "id": "W4CEj-GIqeme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def find_ngrams(self, input_list, n):\n",
        "      return zip(*[input_list[i:] for i in range(n)])"
      ],
      "metadata": {
        "id": "oywYcu11q6Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def cancel_batch(self, batch_id):\n",
        "        \"\"\"Utility to cancel a batch by ID\"\"\"\n",
        "        self.client.batches.cancel(batch_id)"
      ],
      "metadata": {
        "id": "N65397HXq_fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def compute_metrics(self, x):\n",
        "        \"\"\"\n",
        "        - N-grams and unique words\n",
        "        \"\"\"\n",
        "        sequences = [y[\"content\"] for y in x if y[\"role\"] == \"assistant\"]\n",
        "        unique_words = set()\n",
        "        unique_ngrams = set()\n",
        "\n",
        "        for utterance in sequences:\n",
        "            utter = utterance.lower()\n",
        "\n",
        "            for words in utter.split():\n",
        "                unique_words.add(words.strip(string.punctuation))\n",
        "\n",
        "            for i in range(4):\n",
        "                for n in self.find_ngrams(utter.strip(string.punctuation).split(), i+1):\n",
        "                    unique_ngrams.add(n)\n",
        "\n",
        "        return unique_words, unique_ngrams"
      ],
      "metadata": {
        "id": "OOWDwwDbrM01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def create_db(self, db_name : string, UIDs : list):\n",
        "        \"\"\"Convert multiple jsonl files to pickle object for easier transfer & loading\"\"\"\n",
        "\n",
        "        self.change_dir()\n",
        "        PATH = \"../DB/\" + db_name\n",
        "        formatted = []\n",
        "\n",
        "\n",
        "        for UID in UIDs:\n",
        "            agent_name = f\"agent_request_{UID}.jsonl\"\n",
        "\n",
        "            with open(agent_name, 'r', encoding='utf8') as f:\n",
        "                for line in f:\n",
        "                    line = json.loads(line)\n",
        "                    formatted.append(line)\n",
        "\n",
        "        with open(PATH, 'wb') as f:\n",
        "            pickle.dump(formatted, f)\n",
        "\n",
        "        print(f\"Saved to file {PATH}\")"
      ],
      "metadata": {
        "id": "ZVIEhu1xrbEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def timeout(self):\n",
        "        time.sleep(150)"
      ],
      "metadata": {
        "id": "CJ_0Kk63rkTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def load_conversation(self, data):\n",
        "        \"\"\"\n",
        "        - Filter format to determine if conversations stay in character\n",
        "        \"\"\"\n",
        "        prompt = \"\"\n",
        "\n",
        "        for i in range(len(data)-1):\n",
        "            if (i + 1) % 2:\n",
        "                prompt += \"Person A: \" + data[i+1][\"content\"] + \"\\n\"\n",
        "            else:\n",
        "                prompt += \"Person B: \" + data[i+1][\"content\"] + \"\\n\"\n",
        "\n",
        "        return prompt"
      ],
      "metadata": {
        "id": "covn26Jjrm1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def status(self, batch_id):\n",
        "        return self.client.batches.retrieve(batch_id).status"
      ],
      "metadata": {
        "id": "l1O7WB31rpqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def append_jsonl(self, name, dictionary_list):\n",
        "        \"\"\"load string; add msgs; write over original file\"\"\"\n",
        "        self.change_dir()\n",
        "\n",
        "        formatted = []\n",
        "\n",
        "        with open(name, 'r', encoding='utf8') as f:\n",
        "            for line, generated in zip(f, dictionary_list):\n",
        "                line = json.loads(line)\n",
        "                line[\"body\"][\"messages\"].append(generated[\"response\"][\"body\"][\"choices\"][0][\"message\"])\n",
        "                formatted.append(line)\n",
        "\n",
        "        with open(name, 'w', encoding='utf8') as f:\n",
        "            for line in formatted:\n",
        "                json_record = json.dumps(line, ensure_ascii=False)\n",
        "                f.write(json_record + '\\n')"
      ],
      "metadata": {
        "id": "AmPakU_3rxY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def change_dir(self):\n",
        "        \"\"\"Change to batched dir\"\"\"\n",
        "        print(os.getcwd())\n",
        "        if os.getcwd() == self.PATH + \"batched/\":\n",
        "            return\n",
        "\n",
        "        os.chdir(self.PATH + \"batched/\")\n",
        "        print(\"\\n|\\n\" + os.getcwd())"
      ],
      "metadata": {
        "id": "wfm_dI86sDkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def print_content(self, name):\n",
        "        \"\"\"\n",
        "        - utility function to print out batch file's context window\n",
        "        \"\"\"\n",
        "        self.change_dir()\n",
        "\n",
        "        with open(name, 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = json.loads(line)\n",
        "\n",
        "                for x in line[\"body\"][\"messages\"]:\n",
        "                    print(x[\"role\"], \"\\t->\\t\", x[\"content\"], \"\\n\")\n",
        "                print(\"\\n\"*2)"
      ],
      "metadata": {
        "id": "mUXb8Ma_r7LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # NOT NEEDED TO RUN FOR BATCH PROCESSES. SINGLE CONVERSATION GENERATION METHOD.\n",
        "    %%add_to self_talk\n",
        "    def start_generation(self, tr=None, turns=2, utterances=12):\n",
        "        \"\"\"\n",
        "        - Generic single API Requests\n",
        "        - Can be threaded\n",
        "        \"\"\"\n",
        "        accepted = 0\n",
        "        rejections = 0\n",
        "        tokens_in = 0\n",
        "        tokens_out = 0\n",
        "\n",
        "\n",
        "        for i in range(turns):\n",
        "\n",
        "            print(f\"----------------START OF TURN {tr} \\t {i}----------------\")\n",
        "\n",
        "            start,notice,cond = self.convostarter()\n",
        "            client_demographics = self.df.sample_client()\n",
        "            agent_demographics = self.df.sample_agent()\n",
        "\n",
        "            agent_dialogue, client_dialogue = self.generate_prompts2(start, notice, cond, client_demographics, agent_demographics)\n",
        "            # PROMPT IS HERE^\n",
        "            filter_system = f\"\"\"Person A's character is: \"{agent_dialogue[0][\"content\"]}\"\n",
        "\n",
        "            Person B's character is: \"{client_dialogue[0][\"content\"]}\"\n",
        "\n",
        "            Given Person A and Person B characters, determine if this conversation stays within character. Choose either yes or no.\"\"\"\n",
        "\n",
        "\n",
        "            chat_filter = [\n",
        "                {\"role\": \"system\", \"content\": filter_system},\n",
        "            ]\n",
        "\n",
        "\n",
        "            print(\"\\n\\nClient: \\t\", client_dialogue, \"\\n\\nAgent: \\t\", agent_dialogue, \"\\n\\n\")\n",
        "\n",
        "\n",
        "            # print(f\"\\nBeginning of turn {i}.\\n\")\n",
        "\n",
        "            # BEGINNING OF CONVERSATION\n",
        "            for _ in range(utterances):\n",
        "\n",
        "                # API Completion\n",
        "                agent_out = self.client.chat.completions.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    messages=agent_dialogue,\n",
        "                    top_p=self.top_p,\n",
        "                    temperature=self.temp,\n",
        "                    presence_penalty=self.presence,\n",
        "                    frequency_penalty=self.freq\n",
        "                )\n",
        "                print(f\"\\tAgent output: {agent_out.choices[0].message.content}\\n\")\n",
        "\n",
        "                # Addition to history\n",
        "                agent_dialogue.append({\"role\":agent_out.choices[0].message.role, \"content\":agent_out.choices[0].message.content})\n",
        "                client_dialogue.append({\"role\":\"user\", \"content\":agent_out.choices[0].message.content})\n",
        "\n",
        "                # API Completion\n",
        "                client_out = self.client.chat.completions.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    messages=client_dialogue,\n",
        "                    top_p=self.top_p,\n",
        "                    temperature=self.temp,\n",
        "                    presence_penalty=self.presence,\n",
        "                    frequency_penalty=self.freq\n",
        "                )\n",
        "                print(f\"\\tClient output: {client_out.choices[0].message.content}\\n\")\n",
        "                # ADDED THIS 7-3-24\n",
        "                Test = client_out.choices[0].message.content\n",
        "                if \"Adiós\" in Test or \"Hasta\" in Test: continue\n",
        "                # ADDED THIS 7-3-24\n",
        "                # Addition to history\n",
        "                agent_dialogue.append({\"role\":\"user\", \"content\":client_out.choices[0].message.content})\n",
        "                client_dialogue.append({\"role\":client_out.choices[0].message.role, \"content\":client_out.choices[0].message.content})\n",
        "\n",
        "                # Total tokens\n",
        "                tokens_in += client_out.usage.prompt_tokens + agent_out.usage.prompt_tokens\n",
        "                tokens_out += client_out.usage.completion_tokens + agent_out.usage.completion_tokens\n",
        "\n",
        "            # Metrics for ngrams and unique words\n",
        "            uw, un = self.compute_metrics(agent_dialogue)\n",
        "\n",
        "            # Did it stay in character?\n",
        "            chat_filter.append({\"role\": \"user\", \"content\": self.load_conversation(agent_dialogue)})\n",
        "\n",
        "            filter_completion = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=chat_filter\n",
        "                )\n",
        "\n",
        "            # Did it reach end of conversation\n",
        "            EOC = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"At what point does this conversation occur? Choose either the beginning, middle, or end.\"},\n",
        "                    {\"role\": \"user\", \"content\": self.load_conversation(agent_dialogue[-5:])}\n",
        "                ]\n",
        "                )\n",
        "\n",
        "            metrics = {\n",
        "                \"Unique Words\": uw,\n",
        "                \"Unique N-grams\": un,\n",
        "                \"Character\": filter_completion.choices[0].message.content,\n",
        "                \"End of Conversation\": EOC.choices[0].message.content\n",
        "                      }\n",
        "\n",
        "            # print(f\"\\t\\nUnique words: {len(metrics['Unique Words'])}\\nUnique ngrams: {len(metrics['Unique N-grams'])}\\n\",\n",
        "            #      f\"Model stayed in character? {metrics['Character']}\\n\",\n",
        "            #       f\"Model reached EOC? {metrics['End of Conversation']}\\n\"\n",
        "            #      )\n",
        "\n",
        "            # print(\"Filter match?\\t\", (\"end\" in metrics[\"End of Conversation\"].lower()), (\"yes\" in metrics[\"Character\"].lower()))\n",
        "\n",
        "            # Remember the agent turn will have all\n",
        "            # if (\"end\" in metrics[\"End of Conversation\"].lower() and \"yes\" in metrics[\"Character\"].lower()):\n",
        "            #     print(agent_dialogue)\n",
        "            #     accepted += 1\n",
        "            #     with open(self.PATH + f\"accepted/Accepted_Turn_{tr}{i}_{round(time.time() * 1000)}\", \"wb\") as f:\n",
        "            #         pickle.dump({\"Client History\": client_dialogue,\n",
        "            #                      \"Agent History\": agent_dialogue,\n",
        "            #                      \"Metrics\": metrics\n",
        "            #                     },\n",
        "            #                     f)\n",
        "            # else:\n",
        "            #     # Rejection save\n",
        "            #     rejections += 1\n",
        "            #     with open(self.PATH + f\"rejected/Rejected_Turn_{tr}{i}_{round(time.time() * 1000)}\", \"wb\") as f:\n",
        "            #         pickle.dump({\"Client History\": client_dialogue,\n",
        "            #                      \"Agent History\": agent_dialogue,\n",
        "            #                      \"Metrics\": metrics\n",
        "            #                     },\n",
        "            #                     f)\n",
        "\n",
        "\n",
        "            print(f\"{tr} TURN {i}\\nTotal Accepted:Rejections {accepted}:{rejections}\\nTotal tokens in/out: {tokens_in}/{tokens_out}\\nSaved. Moving on to turn {i+1}\\n\", \"\\n\"*4)\n",
        "        print(f\"{tr}: Final tokens in/out: {tokens_in}/{tokens_out}\\n\", \"\\n\"*4)\n"
      ],
      "metadata": {
        "id": "tPL3VtAhsL7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def generate_prompts(self, start, notice, cond, client_demographics, agent_demographics):\n",
        "        \"\"\"\n",
        "        - Generates prompts for both the agent and client.\n",
        "        \"\"\"\n",
        "        agent_dialogue = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are an American airport customs agent. You pulled the user aside because of a suspicion about {agent_demographics['Suspicion']}. You notice they {client_demographics['Att']}{notice}. Ask the user questions in relation to airport security. Your mission is to determine if the user is suspicious from a airport security standpoint. If the user does not answer your questions, {cond}. Only speak in Spanish.\"},\n",
        "            {\"role\": \"user\", \"content\": start}\n",
        "        ]\n",
        "\n",
        "        client_dialogue = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {client_demographics['Age']}-year-old {client_demographics['Nationality']} {client_demographics['Gender']} visiting USA who has {client_demographics['Items']}. You {client_demographics['Att']}. A customs official has taken you aside for further inspection. Only respond in Spanish.\"},\n",
        "        ]\n",
        "\n",
        "        return agent_dialogue, client_dialogue"
      ],
      "metadata": {
        "id": "fdV4zrFl3Wuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def generate_prompts2(self, start, notice, cond, client_demographics, agent_demographics):\n",
        "        \"\"\"\n",
        "        - Generates prompts for both the agent and client.\n",
        "        \"\"\"\n",
        "        agent_dialogue = [\n",
        "            {\"role\": \"system\", \"content\": f\"\"\"You are a customs agent at an American airport. You pulled the user aside because of a suspicion about {agent_demographics['Suspicion']}.\n",
        "            You noticed they {client_demographics['Att']}{notice}. Ask the user questions related to airport security.\n",
        "            Your mission is to determine if the user is suspicious from an airport security standpoint. If the user does not answer your questions, {cond}.\n",
        "            Only speak in Spanish.\n",
        "\n",
        "Here are some additional guidelines:\n",
        "1. If the user apologizes or expresses an unmet need, briefly acknowledge the situation and then shift the topic to a relevant question about airport security. Use templates like:\n",
        "   - \"Entiendo. Ahora, por favor, dígame más sobre [tema relacionado con la seguridad].\"\n",
        "   - \"No hay problema. ¿Puede explicarme [otro aspecto de seguridad]?\"\n",
        "   - \"Gracias por informarme. Ahora, necesito saber sobre [acción sugerida].\"\n",
        "\n",
        "2. Remember that your main goal is to explore and discuss topics related to airport security. If the conversation gets stuck, reintroduce it with a new question or aspect of the topic.\n",
        "\n",
        "3. Vary your responses to keep the conversation dynamic. If you feel you are repeating yourself, introduce a new topic or security question.\n",
        "\n",
        "4. Remember to only speak in Spanish.\n",
        "\n",
        "5. End the conversation with a friendly goodbye.\n",
        "\n",
        "Every five exchanges, remind yourself of the main goal of the conversation: to determine if the user is suspicious from an airport security standpoint. If the conversation gets stuck, reset by introducing a new aspect of the topic or asking a new question.\n",
        "\"\"\"},\n",
        "            {\"role\": \"user\", \"content\": start}\n",
        "        ]\n",
        "\n",
        "        client_dialogue = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {client_demographics['Age']}-year-old {client_demographics['Nationality']} {client_demographics['Gender']} visiting USA who has {client_demographics['Items']}. You {client_demographics['Att']}. A customs official has taken you aside for further inspection. Only respond in Spanish.\"},\n",
        "        ]\n",
        "\n",
        "        return agent_dialogue, client_dialogue"
      ],
      "metadata": {
        "id": "PL2JZWCU-RPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def generate_prompts3(self, start, notice, cond, client_demographics, agent_demographics): # SPANISH PROMPTING, BUT FUNCTIONS TO ADD VARIABILITY ARE IN ENGLISH!!\n",
        "        \"\"\"\n",
        "        - Generates prompts for both the agent and client.\n",
        "        \"\"\"\n",
        "        agent_dialogue = [\n",
        "            {\"role\": \"system\", \"content\": f\"\"\"Eres un agente de aduanas en un aeropuerto de los Estados Unidos. Detuviste al usuario debido a una sospecha sobre {agent_demographics['Suspicion']}. Notaste que {client_demographics['Att']}{notice}.\n",
        "            Haz preguntas al usuario relacionadas con la seguridad del aeropuerto. Tu misión es determinar si el usuario es sospechoso desde el punto de vista de la seguridad del aeropuerto.\n",
        "            Si el usuario no responde a tus preguntas, {cond}. Solo habla en español.\n",
        "\n",
        "Aquí tienes algunas directrices adicionales:\n",
        "1. Si el usuario se disculpa o expresa una necesidad no satisfecha, reconoce brevemente la situación y luego cambia el tema a una pregunta relevante sobre la seguridad del aeropuerto. Usa plantillas como:\n",
        "   - \"Entiendo. Ahora, por favor, dígame más sobre [tema relacionado con la seguridad].\"\n",
        "   - \"No hay problema. ¿Puede explicarme [otro aspecto de seguridad]?\"\n",
        "   - \"Gracias por informarme. Ahora, necesito saber sobre [acción sugerida].\"\n",
        "\n",
        "2. Recuerda que tu objetivo principal es explorar y discutir temas relacionados con la seguridad del aeropuerto. Si la conversación se estanca, reintrodúcelo con una nueva pregunta o aspecto del tema.\n",
        "\n",
        "3. Varía tus respuestas para mantener la conversación dinámica. Si sientes que te estás repitiendo, introduce un nuevo tema o pregunta de seguridad.\n",
        "\n",
        "Cada cinco intercambios, recuérdate a ti mismo el objetivo principal de la conversación: determinar si el usuario es sospechoso desde el punto de vista de la seguridad del aeropuerto. Si la conversación se atasca, restablece introduciendo un nuevo aspecto del tema o haciendo una nueva pregunta.\n",
        "\"\"\"},\n",
        "            {\"role\": \"user\", \"content\": start}\n",
        "        ]\n",
        "\n",
        "        client_dialogue = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {client_demographics['Age']}-year-old {client_demographics['Nationality']} {client_demographics['Gender']} visiting USA who has {client_demographics['Items']}. You {client_demographics['Att']}. A customs official has taken you aside for further inspection. Only respond in Spanish.\"},\n",
        "        ]\n",
        "\n",
        "        return agent_dialogue, client_dialogue"
      ],
      "metadata": {
        "id": "_RT-u6X7A2RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def create_batch(self, amt=1, start=\"Hello.\"): # VIRTUAL FILE IS BEING CREATED FOR EACH REQUEST, FOR BOTH AGENT AND CLIENT. - ML 07/03/24\n",
        "        \"\"\"\n",
        "        - Creates starting batch based on starting phrase and amount of conversations you want to generate\n",
        "        \"\"\"\n",
        "        self.change_dir()\n",
        "\n",
        "        # Creation of jsonl\n",
        "        agent_request = []\n",
        "        client_request = []\n",
        "\n",
        "        # File names\n",
        "        UID = time.strftime('%m-%d-%y_%H%M-%S', time.localtime(time.time())) #CHANGED 8/4/24\n",
        "        agent_name = f\"agent_request_{UID}.jsonl\"\n",
        "        client_name = f\"client_request_{UID}.jsonl\"\n",
        "        print(f\"Requested {amt} prompts to be generated!\")\n",
        "        for i in range(amt):\n",
        "            start,notice,cond = self.convostarter()\n",
        "            client_demographics = self.df.sample_client()\n",
        "            agent_demographics = self.df.sample_agent()\n",
        "\n",
        "            agent_dialogue, client_dialogue = self.generate_prompts2(start, notice, cond, client_demographics, agent_demographics) # NOTE FUNCTION USED TO GENERATE PROMPTS!!\n",
        "            print(f\"Generated prompt: {i}!\")\n",
        "            client_out = self.client.chat.completions.create(model=\"gpt-4o-mini\",messages=client_dialogue)\n",
        "\n",
        "            agent_request.append({\"custom_id\": f\"agent-request-{i}\",\n",
        "             \"method\": \"POST\",\n",
        "             \"url\": \"/v1/chat/completions\",\n",
        "             \"body\": {\"model\": \"gpt-4o-mini\",\n",
        "                      \"messages\": agent_dialogue,\n",
        "                      \"top_p\":self.top_p,\n",
        "                      \"temperature\":self.temp,\n",
        "                      \"presence_penalty\":self.presence,\n",
        "                      \"frequency_penalty\":self.freq,\n",
        "                      \"max_tokens\": 1000}})\n",
        "\n",
        "\n",
        "            client_request.append({\"custom_id\": f\"client-request-{i}\",\n",
        "             \"method\": \"POST\",\n",
        "             \"url\": \"/v1/chat/completions\",\n",
        "             \"body\": {\"model\": \"gpt-4o-mini\",\n",
        "                      \"messages\": client_dialogue,\n",
        "                      \"top_p\":self.top_p,\n",
        "                      \"temperature\":self.temp,\n",
        "                      \"presence_penalty\":self.presence,\n",
        "                      \"frequency_penalty\":self.freq,\n",
        "                      \"max_tokens\": 1000}})\n",
        "\n",
        "        with open(agent_name, 'w', encoding='utf8') as f:\n",
        "            for line in agent_request:\n",
        "                json_record = json.dumps(line, ensure_ascii=False)\n",
        "                f.write(json_record + '\\n')\n",
        "\n",
        "        with open(client_name, 'w', encoding='utf8') as f:\n",
        "            for line in client_request:\n",
        "                json_record = json.dumps(line, ensure_ascii=False)\n",
        "                f.write(json_record + '\\n')\n",
        "\n",
        "        return UID"
      ],
      "metadata": {
        "id": "X4pmiwUbrsp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def convostarter(self):\n",
        "        \"\"\"\n",
        "        - Generates a random opening line for client to use. (ie. instead of always opening with Hola.)\n",
        "        - For creating variability in prompts.\n",
        "        \"\"\"\n",
        "        PhraseList = [\"Hola.\", \"No respondo preguntas.\", \"¿Qué pasa?\", \"Hola, ¿qué tal?\", \"Buenos dias.\", \"Buenos tardes.\", \"Buenas noches.\", \"¿Cómo estás?\", \"¿Cómo te va?\", \"¿Cómo está usted?\", \"Adiós\", \"Hasta Luego.\"]\n",
        "        phrase = random.choice(PhraseList)\n",
        "        if random.randint(0,1):\n",
        "            notice = \", but do not disclose this\"\n",
        "        else:\n",
        "            notice = ''\n",
        "        if random.randint(0,1):\n",
        "            cond = \"do not allow them to leave and call your supervisor\"\n",
        "        else:\n",
        "            cond = \"allow them to leave\"\n",
        "        return phrase, notice, cond"
      ],
      "metadata": {
        "id": "EqyMZmObdKO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %pdb off # FOR TROUBLE SHOOTING"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTU4xAOEKXPt",
        "outputId": "32fbfc7c-5344-4e29-b92c-fd5f4685a656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incorrect argument. Use on/1, off/0, or nothing for a toggle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def generate_batches(self, UID, utterances=1):\n",
        "        \"\"\"\n",
        "        - Submit the jsonl files\n",
        "        - Retrieve the completion file from openai server\n",
        "        - Extract generated output from retrieved file and put into format to append\n",
        "        - Append generated output to jsonl file and continue generation\n",
        "        \"\"\"\n",
        "        self.change_dir()\n",
        "        agent_name = f\"agent_request_{UID}.jsonl\"\n",
        "        client_name = f\"client_request_{UID}.jsonl\"\n",
        "\n",
        "        for i in range(utterances):\n",
        "            print(\"TURN: \", i)\n",
        "\n",
        "            # Agent submit\n",
        "            _, batch_id = self.submit_batch(agent_name)\n",
        "\n",
        "            # Retrieve\n",
        "            file_id = self.client.batches.retrieve(batch_id).output_file_id\n",
        "            content = self.client.files.content(file_id)\n",
        "\n",
        "            agent_res, client_res = self.load_format(content)\n",
        "\n",
        "            self.append_jsonl(agent_name, agent_res)\n",
        "            self.append_jsonl(client_name, client_res)\n",
        "            # FOR LOOP HAULT OPERATION HERE MIGUEL 8/5/24\n",
        "            #ipdb.set_trace()\n",
        "            message_content = agent_res[0]['response']['body']['choices'][0]['message']['content']\n",
        "            if \"Adiós\" in message_content or \"Hasta\" in message_content:\n",
        "              print(f\"Ending phrase in conversation detected. Breaking to new convo.\")\n",
        "              break\n",
        "            # Client submit\n",
        "            _, batch_id = self.submit_batch(client_name) # ERROR HERE\n",
        "\n",
        "            # Retrieve\n",
        "            file_id = self.client.batches.retrieve(batch_id).output_file_id\n",
        "            content = self.client.files.content(file_id)\n",
        "\n",
        "            client_res, agent_res = self.load_format(content)\n",
        "\n",
        "            self.append_jsonl(agent_name, agent_res)\n",
        "            self.append_jsonl(client_name, client_res)"
      ],
      "metadata": {
        "id": "-zg9bjNGr-YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def load_format(self, content):\n",
        "        \"\"\"\n",
        "        - Extracts OpenAI's http response into jsonl format\n",
        "        \"\"\"\n",
        "        normal = []\n",
        "        inverted = []\n",
        "\n",
        "        for x in content.text.split('\\n'):\n",
        "            if x:\n",
        "                normal.append(json.loads(x))\n",
        "\n",
        "        inverted = copy.deepcopy(normal)\n",
        "\n",
        "        for x in inverted:\n",
        "            x[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"role\"] = \"user\"\n",
        "\n",
        "        return normal, inverted #normal is agent_res, inverted is client_res FOR AGENT. for client it is opposite"
      ],
      "metadata": {
        "id": "EVTPKQmZr3PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    %%add_to self_talk\n",
        "    def submit_batch(self, name):\n",
        "        \"\"\"\n",
        "        - Submit jsonl and then await for completion or hang.\n",
        "        \"\"\"\n",
        "        self.change_dir()\n",
        "        print(f\"Attempting to submit batch: {name}\\n\")\n",
        "\n",
        "        batch_input_file = self.client.files.create(\n",
        "        file=open(f\"{name}\", \"rb\"),\n",
        "        purpose=\"batch\"\n",
        "        )\n",
        "\n",
        "        batch_input_file_id = batch_input_file.id\n",
        "\n",
        "        batch_obj = self.client.batches.create(\n",
        "            input_file_id=batch_input_file_id,\n",
        "            endpoint=\"/v1/chat/completions\",\n",
        "            completion_window=\"24h\",\n",
        "            metadata={\n",
        "            \"description\": \"LEIDOS job\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(batch_input_file_id, \"\\n\"*4, batch_obj)\n",
        "        time.sleep(10)\n",
        "\n",
        "        while self.status(batch_obj.id) != 'completed':\n",
        "                currenttime=time.strftime('%m-%d-%y_%H%M-%S', time.localtime(time.time())) #changed 8/4/24\n",
        "                print(currenttime, batch_obj.id, self.status(batch_obj.id))\n",
        "                self.timeout()\n",
        "\n",
        "        return batch_obj, batch_obj.id"
      ],
      "metadata": {
        "id": "P0b1_gAwsG2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    \"\"\"Creating batches\"\"\"\n",
        "    process = self_talk(top_p=0.7,temp=0.8,presence=0.1,PATH=\"/content/drive/MyDrive/Leidos/LEIDOS/\",key=\"sk\")\n",
        "    #PATH=\"E:/LEIDOS/\"\n",
        "\n",
        "\n",
        "\n",
        "    #UID = process.create_batch(50, \"Hola.\") #CHANGED FROM 'NO RESPONDO PREGUNTAS.' - this # is how many prompts generated to send to generate OVERRIDES FUNCTION INPUT IF SPECIFIED HERE\n",
        "    UID = process.create_batch(200)\n",
        "    process.generate_batches(UID, utterances=10) # utterances are \"turns\" in the conversations\n",
        "\n",
        "    # process.cancel_batch('batch_8nHUSrI9pgIfgq9AIJcBJE5W')\n",
        "\n",
        "    \"\"\"Batch Submission\"\"\"\n",
        "    # UID = f\"agent_request_{UID}\" #  ML 8/4/24\n",
        "    # process.submit_batch(UID)\n",
        "    \"\"\"Check Status\"\"\"\n",
        "    # batch_id = \"batch_KUzMeJxc7cc9DQKm1zWgrE45\"\n",
        "    # print(process.status(batch_id))\n",
        "\n",
        "\n",
        "    \"\"\"Single API Requests\"\"\"\n",
        "    #process.start_generation(f\"August5Activity\", 1,10) # 2 total conversations 4 turns each\n",
        "\n",
        "    # threads = []\n",
        "    # amt_threads = 2\n",
        "\n",
        "    # for i in range(amt_threads):\n",
        "    #     threads.append(threading.Thread(target=process.start_generation, args=(f\"T_{i}_buenastardes_\", 200,6)))\n",
        "\n",
        "    # for t in threads:\n",
        "    #     t.start()\n",
        "\n",
        "    # for t in threads:\n",
        "    #     t.join()\n",
        "\n",
        "    # print(\"Threads Joined.\")\n",
        "\n",
        "    \"\"\"Print out content\"\"\"\n",
        "    #process.print_content(\"agent_request_1720039424414.jsonl\")\n",
        "\n",
        "    \"\"\"DB CREATION\"\"\"\n",
        "    # process.create_db(\"unfiltered\", [1716484007823, 1716519250525, 1716586796027, 1716586883138, 1716656236989, 1716656246191])\n"
      ],
      "metadata": {
        "id": "tYptOp6DsbrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49cde82-b717-40b4-fa85-c3d32bc39353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Requested 200 prompts to be generated!\n",
            "Generated prompt: 0!\n",
            "Generated prompt: 1!\n",
            "Generated prompt: 2!\n",
            "Generated prompt: 3!\n",
            "Generated prompt: 4!\n",
            "Generated prompt: 5!\n",
            "Generated prompt: 6!\n",
            "Generated prompt: 7!\n",
            "Generated prompt: 8!\n",
            "Generated prompt: 9!\n",
            "Generated prompt: 10!\n",
            "Generated prompt: 11!\n",
            "Generated prompt: 12!\n",
            "Generated prompt: 13!\n",
            "Generated prompt: 14!\n",
            "Generated prompt: 15!\n",
            "Generated prompt: 16!\n",
            "Generated prompt: 17!\n",
            "Generated prompt: 18!\n",
            "Generated prompt: 19!\n",
            "Generated prompt: 20!\n",
            "Generated prompt: 21!\n",
            "Generated prompt: 22!\n",
            "Generated prompt: 23!\n",
            "Generated prompt: 24!\n",
            "Generated prompt: 25!\n",
            "Generated prompt: 26!\n",
            "Generated prompt: 27!\n",
            "Generated prompt: 28!\n",
            "Generated prompt: 29!\n",
            "Generated prompt: 30!\n",
            "Generated prompt: 31!\n",
            "Generated prompt: 32!\n",
            "Generated prompt: 33!\n",
            "Generated prompt: 34!\n",
            "Generated prompt: 35!\n",
            "Generated prompt: 36!\n",
            "Generated prompt: 37!\n",
            "Generated prompt: 38!\n",
            "Generated prompt: 39!\n",
            "Generated prompt: 40!\n",
            "Generated prompt: 41!\n",
            "Generated prompt: 42!\n",
            "Generated prompt: 43!\n",
            "Generated prompt: 44!\n",
            "Generated prompt: 45!\n",
            "Generated prompt: 46!\n",
            "Generated prompt: 47!\n",
            "Generated prompt: 48!\n",
            "Generated prompt: 49!\n",
            "Generated prompt: 50!\n",
            "Generated prompt: 51!\n",
            "Generated prompt: 52!\n",
            "Generated prompt: 53!\n",
            "Generated prompt: 54!\n",
            "Generated prompt: 55!\n",
            "Generated prompt: 56!\n",
            "Generated prompt: 57!\n",
            "Generated prompt: 58!\n",
            "Generated prompt: 59!\n",
            "Generated prompt: 60!\n",
            "Generated prompt: 61!\n",
            "Generated prompt: 62!\n",
            "Generated prompt: 63!\n",
            "Generated prompt: 64!\n",
            "Generated prompt: 65!\n",
            "Generated prompt: 66!\n",
            "Generated prompt: 67!\n",
            "Generated prompt: 68!\n",
            "Generated prompt: 69!\n",
            "Generated prompt: 70!\n",
            "Generated prompt: 71!\n",
            "Generated prompt: 72!\n",
            "Generated prompt: 73!\n",
            "Generated prompt: 74!\n",
            "Generated prompt: 75!\n",
            "Generated prompt: 76!\n",
            "Generated prompt: 77!\n",
            "Generated prompt: 78!\n",
            "Generated prompt: 79!\n",
            "Generated prompt: 80!\n",
            "Generated prompt: 81!\n",
            "Generated prompt: 82!\n",
            "Generated prompt: 83!\n",
            "Generated prompt: 84!\n",
            "Generated prompt: 85!\n",
            "Generated prompt: 86!\n",
            "Generated prompt: 87!\n",
            "Generated prompt: 88!\n",
            "Generated prompt: 89!\n",
            "Generated prompt: 90!\n",
            "Generated prompt: 91!\n",
            "Generated prompt: 92!\n",
            "Generated prompt: 93!\n",
            "Generated prompt: 94!\n",
            "Generated prompt: 95!\n",
            "Generated prompt: 96!\n",
            "Generated prompt: 97!\n",
            "Generated prompt: 98!\n",
            "Generated prompt: 99!\n",
            "Generated prompt: 100!\n",
            "Generated prompt: 101!\n",
            "Generated prompt: 102!\n",
            "Generated prompt: 103!\n",
            "Generated prompt: 104!\n",
            "Generated prompt: 105!\n",
            "Generated prompt: 106!\n",
            "Generated prompt: 107!\n",
            "Generated prompt: 108!\n",
            "Generated prompt: 109!\n",
            "Generated prompt: 110!\n",
            "Generated prompt: 111!\n",
            "Generated prompt: 112!\n",
            "Generated prompt: 113!\n",
            "Generated prompt: 114!\n",
            "Generated prompt: 115!\n",
            "Generated prompt: 116!\n",
            "Generated prompt: 117!\n",
            "Generated prompt: 118!\n",
            "Generated prompt: 119!\n",
            "Generated prompt: 120!\n",
            "Generated prompt: 121!\n",
            "Generated prompt: 122!\n",
            "Generated prompt: 123!\n",
            "Generated prompt: 124!\n",
            "Generated prompt: 125!\n",
            "Generated prompt: 126!\n",
            "Generated prompt: 127!\n",
            "Generated prompt: 128!\n",
            "Generated prompt: 129!\n",
            "Generated prompt: 130!\n",
            "Generated prompt: 131!\n",
            "Generated prompt: 132!\n",
            "Generated prompt: 133!\n",
            "Generated prompt: 134!\n",
            "Generated prompt: 135!\n",
            "Generated prompt: 136!\n",
            "Generated prompt: 137!\n",
            "Generated prompt: 138!\n",
            "Generated prompt: 139!\n",
            "Generated prompt: 140!\n",
            "Generated prompt: 141!\n",
            "Generated prompt: 142!\n",
            "Generated prompt: 143!\n",
            "Generated prompt: 144!\n",
            "Generated prompt: 145!\n",
            "Generated prompt: 146!\n",
            "Generated prompt: 147!\n",
            "Generated prompt: 148!\n",
            "Generated prompt: 149!\n",
            "Generated prompt: 150!\n",
            "Generated prompt: 151!\n",
            "Generated prompt: 152!\n",
            "Generated prompt: 153!\n",
            "Generated prompt: 154!\n",
            "Generated prompt: 155!\n",
            "Generated prompt: 156!\n",
            "Generated prompt: 157!\n",
            "Generated prompt: 158!\n",
            "Generated prompt: 159!\n",
            "Generated prompt: 160!\n",
            "Generated prompt: 161!\n",
            "Generated prompt: 162!\n",
            "Generated prompt: 163!\n",
            "Generated prompt: 164!\n",
            "Generated prompt: 165!\n",
            "Generated prompt: 166!\n",
            "Generated prompt: 167!\n",
            "Generated prompt: 168!\n",
            "Generated prompt: 169!\n",
            "Generated prompt: 170!\n",
            "Generated prompt: 171!\n",
            "Generated prompt: 172!\n",
            "Generated prompt: 173!\n",
            "Generated prompt: 174!\n",
            "Generated prompt: 175!\n",
            "Generated prompt: 176!\n",
            "Generated prompt: 177!\n",
            "Generated prompt: 178!\n",
            "Generated prompt: 179!\n",
            "Generated prompt: 180!\n",
            "Generated prompt: 181!\n",
            "Generated prompt: 182!\n",
            "Generated prompt: 183!\n",
            "Generated prompt: 184!\n",
            "Generated prompt: 185!\n",
            "Generated prompt: 186!\n",
            "Generated prompt: 187!\n",
            "Generated prompt: 188!\n",
            "Generated prompt: 189!\n",
            "Generated prompt: 190!\n",
            "Generated prompt: 191!\n",
            "Generated prompt: 192!\n",
            "Generated prompt: 193!\n",
            "Generated prompt: 194!\n",
            "Generated prompt: 195!\n",
            "Generated prompt: 196!\n",
            "Generated prompt: 197!\n",
            "Generated prompt: 198!\n",
            "Generated prompt: 199!\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  0\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-EseMHLM1RY3a3Z931GNX3xva \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_TRMiGeLsnnuxE7Dj8ZlYKTO9', completion_window='24h', created_at=1723003530, endpoint='/v1/chat/completions', input_file_id='file-EseMHLM1RY3a3Z931GNX3xva', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723089930, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2305-41 batch_TRMiGeLsnnuxE7Dj8ZlYKTO9 in_progress\n",
            "08-06-24_2308-11 batch_TRMiGeLsnnuxE7Dj8ZlYKTO9 in_progress\n",
            "08-06-24_2310-42 batch_TRMiGeLsnnuxE7Dj8ZlYKTO9 in_progress\n",
            "08-06-24_2313-12 batch_TRMiGeLsnnuxE7Dj8ZlYKTO9 in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-D1WFkLTqvesyEc5aQ5IWm2RO \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_r8EbExlg4j32KSQShAOMWMQr', completion_window='24h', created_at=1723004144, endpoint='/v1/chat/completions', input_file_id='file-D1WFkLTqvesyEc5aQ5IWm2RO', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723090544, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2315-54 batch_r8EbExlg4j32KSQShAOMWMQr in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  1\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-B82zSWXwWztjRSNbJji2zOY1 \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_HTz4cMjUl1EG5HdADZ4dvpj2', completion_window='24h', created_at=1723004307, endpoint='/v1/chat/completions', input_file_id='file-B82zSWXwWztjRSNbJji2zOY1', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723090707, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2318-37 batch_HTz4cMjUl1EG5HdADZ4dvpj2 in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-d8NFILbQsCOZkFhApslFOZcn \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_orot65ibN3MVoeaVI1ZvqHW4', completion_window='24h', created_at=1723004470, endpoint='/v1/chat/completions', input_file_id='file-d8NFILbQsCOZkFhApslFOZcn', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723090870, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2321-20 batch_orot65ibN3MVoeaVI1ZvqHW4 in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  2\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-NKcRJkTZVyzp1zEmlCv2wLAk \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_9pcUcvZrixfPm7wYLuunAprg', completion_window='24h', created_at=1723004632, endpoint='/v1/chat/completions', input_file_id='file-NKcRJkTZVyzp1zEmlCv2wLAk', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723091032, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2324-03 batch_9pcUcvZrixfPm7wYLuunAprg in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-8eM1UsgJzTwNSQGj2qSqwFEz \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_lLl9dDNp2maLX7C4CvsxjT4q', completion_window='24h', created_at=1723004795, endpoint='/v1/chat/completions', input_file_id='file-8eM1UsgJzTwNSQGj2qSqwFEz', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723091195, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2326-45 batch_lLl9dDNp2maLX7C4CvsxjT4q in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  3\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-CWkOwgtnxlE9D91bIj9QIt5R \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_euNBnEAwvNEfQgIyrs1fKQRU', completion_window='24h', created_at=1723004958, endpoint='/v1/chat/completions', input_file_id='file-CWkOwgtnxlE9D91bIj9QIt5R', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723091358, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2329-28 batch_euNBnEAwvNEfQgIyrs1fKQRU in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-46rfnxb8Wn0iSvg02f7BjWVn \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_8FL1UBc0qOXK2NHhNXLeksN6', completion_window='24h', created_at=1723005120, endpoint='/v1/chat/completions', input_file_id='file-46rfnxb8Wn0iSvg02f7BjWVn', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723091520, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2332-10 batch_8FL1UBc0qOXK2NHhNXLeksN6 in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  4\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-xzGyLsbm9Psk2QoL1NWHURxM \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_XST0Tiszy61mHydZF55uTvY4', completion_window='24h', created_at=1723005282, endpoint='/v1/chat/completions', input_file_id='file-xzGyLsbm9Psk2QoL1NWHURxM', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723091682, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2334-53 batch_XST0Tiszy61mHydZF55uTvY4 in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-60DEdYquoqJ60fHwfj0L3yge \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_17Mbx3A2hWCQ7bOLWzgKeB2F', completion_window='24h', created_at=1723005445, endpoint='/v1/chat/completions', input_file_id='file-60DEdYquoqJ60fHwfj0L3yge', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723091845, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2337-35 batch_17Mbx3A2hWCQ7bOLWzgKeB2F in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  5\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-LGjJwbPbXkAOokWou9Jp7gEL \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_symdnzKuRI0FBFUZOvJJXYBU', completion_window='24h', created_at=1723005608, endpoint='/v1/chat/completions', input_file_id='file-LGjJwbPbXkAOokWou9Jp7gEL', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723092008, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2340-18 batch_symdnzKuRI0FBFUZOvJJXYBU in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-Ks7eGeKYMnXvhfg8AbOUhfcs \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_sOwINDrhGieuWDT50t3cwgR8', completion_window='24h', created_at=1723005771, endpoint='/v1/chat/completions', input_file_id='file-Ks7eGeKYMnXvhfg8AbOUhfcs', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723092171, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2343-01 batch_sOwINDrhGieuWDT50t3cwgR8 in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  6\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-yExWk9T6fBWwvW453LjMWhgi \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_jOrcySkJJeBBLQEW5Givf6SE', completion_window='24h', created_at=1723005933, endpoint='/v1/chat/completions', input_file_id='file-yExWk9T6fBWwvW453LjMWhgi', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723092333, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2345-43 batch_jOrcySkJJeBBLQEW5Givf6SE in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-WQAgsAnLwXKsbnY9HKQUiV4T \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_TXGmwoTHNfhZ8SRNDhaCNEbN', completion_window='24h', created_at=1723006096, endpoint='/v1/chat/completions', input_file_id='file-WQAgsAnLwXKsbnY9HKQUiV4T', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723092496, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2348-26 batch_TXGmwoTHNfhZ8SRNDhaCNEbN in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  7\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-OB1BCxNhEiysq6F6yOXZSKva \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_riekRIiXDRnEhCuZBM3CZc1P', completion_window='24h', created_at=1723006258, endpoint='/v1/chat/completions', input_file_id='file-OB1BCxNhEiysq6F6yOXZSKva', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723092658, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-06-24_2351-08 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-06-24_2353-39 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-06-24_2356-09 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-06-24_2358-40 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0001-10 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0003-40 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0006-11 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0008-41 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0011-11 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0013-42 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "08-07-24_0016-12 batch_riekRIiXDRnEhCuZBM3CZc1P in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-dcMcAHBYD84b4o5r0Ipi002S \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_RTfLWREvyvEU7KiwKH30EHkf', completion_window='24h', created_at=1723007925, endpoint='/v1/chat/completions', input_file_id='file-dcMcAHBYD84b4o5r0Ipi002S', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723094325, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-07-24_0018-55 batch_RTfLWREvyvEU7KiwKH30EHkf validating\n",
            "08-07-24_0021-26 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0023-56 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0026-26 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0028-57 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0031-27 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0033-57 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0036-28 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0038-58 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0041-28 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0043-59 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0046-29 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0048-59 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0051-30 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0054-00 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0056-31 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0059-01 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0101-32 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0104-02 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0106-33 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0109-03 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0111-34 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0114-04 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0116-34 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "08-07-24_0119-05 batch_RTfLWREvyvEU7KiwKH30EHkf in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  8\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-ffb0c4Dl94CEJydeOwkv6AEs \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_8CPr6CXWyqKHRBMSUIOODA8Y', completion_window='24h', created_at=1723011704, endpoint='/v1/chat/completions', input_file_id='file-ffb0c4Dl94CEJydeOwkv6AEs', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723098104, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-07-24_0121-55 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0124-25 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0126-55 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0129-26 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0131-56 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0134-27 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0136-57 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0139-27 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0141-58 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0144-28 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0146-59 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "08-07-24_0149-29 batch_8CPr6CXWyqKHRBMSUIOODA8Y in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: client_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-XoYK5oyXTLQcd0sVVCTqTfGY \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_mGIHq1q8YDIvoMgsAJ2aLMAn', completion_window='24h', created_at=1723013522, endpoint='/v1/chat/completions', input_file_id='file-XoYK5oyXTLQcd0sVVCTqTfGY', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723099922, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-07-24_0152-12 batch_mGIHq1q8YDIvoMgsAJ2aLMAn in_progress\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "TURN:  9\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Attempting to submit batch: agent_request_08-06-24_2303-00.jsonl\n",
            "\n",
            "file-3VQm2d2IWfVN8dC1MgLS2n02 \n",
            "\n",
            "\n",
            "\n",
            " Batch(id='batch_NRsdCmuOOeBYvM62FdUEWamB', completion_window='24h', created_at=1723013685, endpoint='/v1/chat/completions', input_file_id='file-3VQm2d2IWfVN8dC1MgLS2n02', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1723100085, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'LEIDOS job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
            "08-07-24_0154-55 batch_NRsdCmuOOeBYvM62FdUEWamB in_progress\n",
            "08-07-24_0157-25 batch_NRsdCmuOOeBYvM62FdUEWamB finalizing\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "\n",
            "|\n",
            "/content/drive/MyDrive/Leidos/LEIDOS/batched\n",
            "Ending phrase in conversation detected. Breaking to new convo.\n"
          ]
        }
      ]
    }
  ]
}