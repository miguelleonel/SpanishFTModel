#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# -*- coding: utf-8 -*-
"""llama-fine-tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Vz3oDdJAPdbMXTD_X4lXN_sufokbDJd
"""

#get_ipython().system('pip install -q -U bitsandbytes')
#get_ipython().system('pip install -q -U git+https://github.com/huggingface/transformers.git')
#get_ipython().system('pip install -q -U git+https://github.com/huggingface/peft.git')
#get_ipython().system('pip install -q -U datasets')
#get_ipython().system('pip install -q -U trl')


# In[1]:


import torch
torch.__version__


# In[2]:


"""# Normal Padding"""

# Bring in ready samples

import os, pickle


PATH = './pickles/'
len(os.listdir(PATH))

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login, logout
login("hf_")

#Both this and next bit error to "conversations roles must alternate user/assistant/user/
checkpoint = 'meta-llama/Llama-2-7b-chat-hf'
tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding='right')
# tokenizer.add_special_tokens({'pad_token':'[PAD]'})
tokenizer.pad_token = tokenizer.unk_token 

def load(name):
  with open(name, "rb") as f:
      data = pickle.load(f)
  return tokenizer.apply_chat_template(data["Agent History"], tokenize=False)

df = []

for idx, x in enumerate(os.listdir(PATH)):
    df.append({"text": load(PATH + x)})
'''
checkpoint = 'meta-llama/Llama-2-7b-chat-hf'
tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding='right')
tokenizer.pad_token = tokenizer.unk_token 

def load(name):
    with open(name, "rb") as f:
        data = pickle.load(f)
    
    # Process each dictionary in the list
    processed_texts = []
    if isinstance(data, list):
        for entry in data:
            if "Agent History" in entry:
                result = tokenizer.apply_chat_template(entry["Agent History"], tokenize=False)
                processed_texts.append(result)
            else:
                print("Key 'Agent History' not found in the dictionary.")
    else:
        print("Expected data to be a list.")
    
    return processed_texts
'''
df = []

# Assuming PATH is defined
for idx, x in enumerate(os.listdir(PATH)):
    file_path = os.path.join(PATH, x)
    df.append({"text": load(file_path)})



# len(df)

# df[0]

# df_list = list()
# for x in df:
#     current = tokenizer(x, return_length=True)
#     current['text'] = x
#     df_list.append(current) 


# In[3]:


import torch
from datasets import Dataset
from trl import DataCollatorForCompletionOnlyLM

dataset = Dataset.from_list(df)
response_template = '[/INST]'
instruction_template = '[INST]'
collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)

# dataloader = torch.utils.data.DataLoader(dataset=dataset,
                                         # collate_fn=collator)

# max_seq_length = 0

# for x in dataloader:
  # for y in x['input_ids']:
    # max_seq_length = max(len(y),max_seq_length)


# In[4]:


dataset


# In[5]:


# dataset['text'] = 0
"""# Model"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from trl import setup_chat_format

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    checkpoint,
    device_map="auto"
)


# In[6]:


# # set chat template to OAI chatML, remove if you start from a fine-tuned model
# model, tokenizer = setup_chat_format(model, tokenizer)

"""# PEFT Set-up"""

from peft import LoraConfig

# LoRA config based on QLoRA paper & Sebastian Raschka experiment
peft_config = LoraConfig(
        lora_alpha=128,
        lora_dropout=0.05,
        r=256,
        bias="none",
        target_modules="all-linear",
        task_type="CAUSAL_LM",
)


# In[7]:


from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="llama-2-7b-spanish-airport", # directory to save and repository id
    num_train_epochs=1,                     # number of training epochs
    per_device_train_batch_size=3,          # batch size per device during training
    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass
    optim="adamw_torch",              # use fused adamw optimizer
    logging_steps=10,                       # log every 10 steps
    save_strategy="epoch",                  # save checkpoint every epoch
    learning_rate=2e-4,                     # learning rate, based on QLoRA paper
    fp16=True,                              # use bfloat16 precision
    # tf32=True,                              # use tf32 precision
    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper
    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper
    lr_scheduler_type="constant",           # use constant learning rate scheduler
    group_by_length=True,
    length_column_name='length',
)


# In[8]:


from trl import SFTTrainer

max_seq_length = 2048 # max sequence length for model and packing of the dataset

trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=dataset,
    dataset_text_field='text',
    data_collator=collator,
    peft_config=peft_config,
    max_seq_length=max_seq_length
)


# In[9]:


#get_ipython().system('nvidia-smi')


# In[ ]:


# start training, the model will be automatically saved to the hub and the output directory
trainer.train()


# In[ ]:


# save model
trainer.save_model()


# In[ ]:


# free the memory again
#del model
#del trainer


# In[ ]:


#torch.cuda.empty_cache()


# In[ ]:


context_window = [
    {"role": "system", "content": "Your name is Rosamond Solum. You are a 21-year-old woman who is a Italian airport customs agent. You pulled the user aside because of a suspicion about being on a wanted list. Ask the user questions in relation to airport security. Your mission is to determine if the user is suspicious from an airport security standpoint. Only speak in Spanish."}, 
    {"role": "user", "content": "¿Cómo está?"}
]


# In[ ]:


#tokenizer.apply_chat_template(context_window, tokenize=False)


# In[ ]:


#tokenizer


# In[ ]:


#pipe.tokenizer.eos_token_id


# In[ ]:


prompt = pipe.tokenizer.apply_chat_template(context_window, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)


# In[ ]:


print(outputs)


# In[ ]:


#x = tokenizer.apply_chat_template(context_window, add_generation_prompt=True, return_tensors='pt')
#x = model.generate(x, max_new_tokens=64)


# In[ ]:


#tokenizer.decode(x[0])


# In[ ]:





